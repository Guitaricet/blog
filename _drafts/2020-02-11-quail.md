---
layout: post
slug: quail
author: arogers
title:  "Question Answering for Artificial Intelligence (QuAIL)"
date:   2020-02-11 09:00:47
tags: qa 
mathjax: false
toc: true
excerpt: "QuAIL is a new challenging NLP benchmark that combines reading comprehension and commonsense reasoning."
summary_image: /assets/images/bert-secrets-header-square.png 
---

> This blog post summarizes our AAAI 2020 paper "Getting Closer to AI-complete Question Answering: A Set of Prerequisite Real Tasks" {% cite RogersKovalevaEtAl_2020_Getting_Closer_to_AI_Complete_Question_Answering_Set_of_Prerequisite_Real_Tasks %}. Paper PDF: [https://aaai.org/Papers/AAAI/2020GB/AAAI-RogersA.7778.pdf](https://aaai.org/Papers/AAAI/2020GB/AAAI-RogersA.7778.pdf) 

Since 2018 NLP saw an explosion of new verbal reasoning datasets: reading comprehension, commonsense reasoning, natural language inference. Simultaneously with that explosion there came a long stream of papers raising concerns about data artifacts and biases, which enable models achieve seemingly super-human accuracy without any real verbal reasoning skills.

One of the top reasons for the datasets being so easy is poor diversity of data that is generated in large amounts by crowd workers with the same kind of prompts and instructions. This may result in large portions of data exhibiting spurious patterns that the model learns to strongly associate with a particular label (such as "never" strongly associated with the contradiction label in SNLI {% cite GururanganSwayamdiptaEtAl_2018_Annotation_Artifacts_in_Natural_Language_Inference_Data%}). If a few such patterns cover large portions of the data, you may get high accuracy with no linguistic competence.

To address the problem, fundamentally we need to reduce the amount of spurious correlations with predicted labels, which would hopefully force our models to learn generalizable patterns rather than dataset-specific "shortcuts". Currently the community is exploring the following approaches to achieving that:

* adversarial authoring with a model-in-the-loop {% cite DuaWangEtAl_2019_DROP_Reading_Comprehension_Benchmark_Requiring_Discrete_Reasoning_Over_Paragraphs %}
* ensembling datasets {% cite DuaGottumukkalaEtAl_2019_ORB_Open_Reading_Benchmark_for_Comprehensive_Evaluation_of_Machine_Reading_Comprehension %}
* partitioning the dataset data into balanced subsections written with diverse prompts, which should decrease the likelihood of a single bias affecting a large portion of data. 

The obvious solution is to aim for more diversity, and to partition the data in ways that would enable more control over what is collected, and how the models are handling it. This is what we attempted to do in QuAIL. In particular, we were interested having a wide selection of questions according to the following criteria:

* **multi-domain RC**: QuAIL combines news, blogs, fiction and user stories, each represented by 200 CC-licensed hand-picked texts 300-350 words long.
* **question type balance**: most crowdsourced RC datasets provide the workers with minimal instructions and only analyze a small sample of the generated questions to see what question types they got, or limit such analysis to simple statistics by the first word of the question. Since crowd workers generally aim to perform task as quickly as possible with the least amount of effort, this may lead to the most obvious types of questions over-represented at the expense of others, producing the impression that the models are doing better than they are.
* **question type annotation**: we show that it is possible to generate questions of specific types and collect question type annotations, which enable fine-grained diagnostics of the model performance.
* **open and closed-world questions**: while many RC datasets focus on Wikipedia and news (texts that describe facts likely known from other sources and available in pretraining data of models like BERT), QuAIL's fiction and user stories texts can be assumed to describe unique situations.
* reasoning across the **full range of uncertaingy**: RC datasets typically assume that the answer is to be found or not in the provided documents, and commonsense reasoning datasets assume that the model needs to have some kind of extra knowledge resources it could use. QuAIL is the first dataset to combine text-based, world knowledge and unanswerable questions. In this setting, **the model has to know when it can find an answer, when it could make a confident guess, and when guessing would not be fruitful**.

That was a lot of things to try in just one dataset, but we learned a lot that could be helpful for future RC research. Here are the main points.

#
