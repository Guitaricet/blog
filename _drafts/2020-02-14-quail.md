---
layout: post
slug: quail
author: arogers
title:  "Question Answering for Artificial Intelligence (QuAIL)"
date:   2020-02-14 09:00:47
tags: QA, transformers 
mathjax: false
toc: true
excerpt: "QuAIL is a new challenging NLP benchmark that combines reading comprehension and commonsense reasoning."
summary_image: /assets/images/quail-card-square.jpg 
---

> This blog post summarizes our AAAI 2020 paper "Getting Closer to AI-complete Question Answering: A Set of Prerequisite Real Tasks" {% cite RogersKovalevaEtAl_2020_Getting_Closer_to_AI_Complete_Question_Answering_Set_of_Prerequisite_Real_Tasks %}. <br/> Paper PDF: [https://aaai.org/Papers/AAAI/2020GB/AAAI-RogersA.7778.pdf](https://aaai.org/Papers/AAAI/2020GB/AAAI-RogersA.7778.pdf) 

Since 2018 there was an explosion of new verbal reasoning datasets: reading comprehension, commonsense reasoning, natural language inference. However, many datasets got "solved" almost immediately. This suspicious ease prompted concerns about data artifacts and biases, which enable models achieve seemingly super-human accuracy without any real verbal reasoning skills.

One of the top reasons for the datasets being so easy is poor diversity of data. Deep learning is data-hungry, which means that the datasets tend to be large, which typically means that it is generated by crowdworkers that provided with the same kind of prompts and minimal instructions. This may result in large portions of data exhibiting spurious patterns that the model learns to associate with a particular label. For example, the word "never" is strongly predictive of the contradiction label in SNLI {% cite GururanganSwayamdiptaEtAl_2018_Annotation_Artifacts_in_Natural_Language_Inference_Data%}, simply because negating was an easy strategy for the crowd workers to generate contradictory statements. If a few such patterns cover large portions of the data, a model may get high accuracy with no linguistic competence.

To address the problem, fundamentally we need to reduce the amount of spurious correlations with predicted labels, which would hopefully force our models to learn generalizable patterns rather than dataset-specific "shortcuts". Currently the community is exploring the following approaches to achieving that:

* **adversarial authoring** with a model-in-the-loop {% cite DuaWangEtAl_2019_DROP_Reading_Comprehension_Benchmark_Requiring_Discrete_Reasoning_Over_Paragraphs %}
* **multi-dataset collections** instead of individual datasets {% cite DuaGottumukkalaEtAl_2019_ORB_Open_Reading_Benchmark_for_Comprehensive_Evaluation_of_Machine_Reading_Comprehension %}, so that the model would be forced to generalize between different kinds of data.

However, both of these approaches have caveats. Combining different datasets means that the questions are specific to the texts in those datasets: if the texts are not matched by domain, length and discourse structure, and come with different question distributions, the model could learn what kinds of questions are asked of what texts. As for adversarial authoring, its success ultimately depends on the hypothesis of a specific model-in-the-loop, which may not generalize to other models. For instance, the authors of PIQA used BERT as the adversary, and in the final dataset it performed worse than GPT-2 {% cite BiskZellersEtAl_2020_PIQA_Reasoning_about_Physical_Commonsense_in_Natural_Language %}, although generally it outperforms GPT-2 on many other benchmarks. There is also recent evidence that stronger adversaries change the question distribution in a way that prevents some weaker models from generalizing to the original distribution {% cite BartoloRobertsEtAl_2020_Beat_AI_Investigating_Adversarial_Human_Annotations_for_Reading_Comprehension %}.

QuAIL was developed concurrently with the proposals for adversarial and multi-dataset approaches, and it explores an alternative solution to the data diversity problem: **partitioning the dataset into balanced subsections written with diverse prompts**. This should decrease the likelihood of a single bias affecting a large portion of data. In particular, we experiment with 4x9 design: 4 domains by 9 question types, each with approximately the same number of questions. A crucial part of our work that was not done in most other datasets is that QuAIL provides annotation for both question types and domains, which enables diagnostics of both the models and the data. When we started working on QuAIL, the only other dataset that allowed that was the original bAbI {% cite WestonBordesEtAl_2015_Towards_AI-complete_question_answering_A_set_of_prerequisite_toy_tasks %}, but it consisted of toy tasks with synthetic data.

It turns out, there are very good reasons why this approach has not been explored before. We have learned a lot from this project, and here are the main takeaways:

> * partitioning and balancing data tells a lot about both the models and the data;
> * generating specific question types is hard, but not impossible;
> * reasoning over the full spectrum of uncertainty is harder for humans than for machines;
> * paraphrasing hurts even BERT.

## Partitioned data is great!

QuAIL is a multi-domain dataset featuring news, blogs, fiction and user stories. Each domain is represented by 200 texts, which gives us a 4-way data split. The texts are CC-licensed and 300-350 words long: excerpts that were hand-picked so as to make sense to human readers without larger context. QuAIL is a multi-choice dataset where all questions have 3 answer options + "not enough information" option that is the correct answer for the unanswerable questions.

<!-- Note that most current SOTA systems are based on large pre-trained models, which raises the issue of possible overlap between pre-training data and the task dataset. In particular, BERT is trained on Wikipedia + BookCorpus, but the authors still tested it on SQuAD {% cite DevlinChangEtAl_2019_BERT_Pre-training_of_Deep_Bidirectional_Transformers_for_Language_Understanding %}. In QuAIL, two of the domains (news and, to some degree, blogs) contain information that is factual and *could* be found in other texts in a large pretraining corpus. However, fiction (by recent and not well-known writers) and personal stories shared on Quora could be assumed to describe unique combinations of events and characters. -->

In addition to the domains, we have 9 question types in 3 categories: 

- **text-based questions** (the information should be found in the text):
  * reasoning about factual information in the text (e.g. *What did John do?*);
  * temporal order of events (e.g. *When did X happen - before, after, or during Y?*);
  * character identity (e.g. *Who ate the cake?*);
- **world knowledge questions** (typically some kind of inference about characters and events, based on information in the text and world knowledge which should make one answer option more likely than others):
  * causality (e.g. *Why did John eat the cake?*);
  * properties and qualities of characters (e.g. *What kind of food does John probably like?*);
  * belief states (e.g. *What does John think about Mary?*); 
  * subsequent state after the narrated events (e.g. *What does John do next?*);
  * duration of the narrated events (e.g. *How long did it probably take John to eat the cake?*);
- **unanswerable questions** are questions for which the information is not found in the text, and all answer options are equally likely (e.g. *What is John's brother's name?*)

This gives us the total of 4 x 9 balanced subsets of the data, which come with annotations that could be used for diagnosing both the models (what they can and can't do) and the data (finding the sections that are suspiciously easy). 

The random choice accuracy is 25%. Let us see what we can get with a simple heuristic baseline (longest answer), an LSTM using word embeddings, BERT, and TriAN - a ConceptNet-based system that won SemEval2018-Task11 {% cite wang-etal-2018-yuanfudao %}.

<div class="table-wrapper" markdown="block">

| Question type          | LongChoice | LSTM | PMI    | TriAN  | BERT   |
|------------------------|------------|------|--------|--------|--------|
| Temporal order         | 36.3       | 37   | 42.5   | **55.5** | 52.9   |
| Character identity      | 32.3       | 32.4 | 48.3   | **53.1** | 46.2   |
| Causality              | 46.8       | 38.5 | 57.8   | 60.1   | **67.1** |
| Factual                | 35.9       | 20.2 | **57.5** | 55     | 55.8   |
| Subsequent state       | 29.5       | 36.8 | 32.9   | 47.5   | **56.7** |
| Event duration         | 33.6       | 43.6 | 37     | 56.9   | **63.8** |
| Entity properties      | 35         | 30.8 | 33.7   | 45.8   | **48.8** |
| Belief states          | 30.9       | 34.7 | 37.5   | 43.3   | **55.0** |
| Unanswerable questions | 12.2       | 51.8 | 23.3   | 65     | 54.2   |
| All questions          | 35.6       | 37.2 | 41.8   | 54.7   | **55.9** |

</div>


The first fun fact is that on world knowledge questions BERT beats a ConceptNet-based system made specifically for this kind of multi-chocie QA, which suggests either that it contains more of world knowledge than ConceptNet, or its world knowledge is more relevant to QuAIL. 

The most difficult question type for BERT is character identity, which often involves coreference resolution. Although both coreference and temporal order questions involve reasoning over longer dependencies, the former appears to be more difficult. In both cases, it loses to TriAN.

Surprisingly, simple PMI is sufficient to beat BERT on factual questions, which suggests that it does do something more complex than cooccurrence counts, but that strategy fails it sometimes. Note that factual questions like (e.g. *What did John eat?*) are the most prevalent type in most over RC datasets. If simple lexical matching provides such a strong signal, then training on data where this kind of questions prevails would teach a model to rely on it.

Finally, note that having partitioned data in conjunction with heuristic baselines helps to find biases. In particular, our LongChoice heuristic is very successful on causality questions, which suggests that the Turkers mostly wrote the long causal explanations for the correct answers but not distractors. Since BERT is the most successful on causality questions, it's possible that this bias helps it as well. For the dataset authors, partitioning means the ability to easily find the problematic spots and to fix them, e.g. making the next version of QuAIL much more difficult. 

Partitioning across domains is helpful as well. Consider the following breakdown of BERT results:

<div class="table-wrapper" markdown="block">

| Question category| Fiction | News    | Blogs | User stories |
|------------------|---------|---------|-------|--------------|
| Text-based       | 45.5    | 38.8    | 60.5  | **61.6**     |
| World knowledge  | **61**  | 58      | 58.3  | 55.6         |
| Unanswerable     | 58.3    | **68.3**| 40    | 50           |
| All questions    | 55.5    | 52.7    | **57**| **57**       |

</div>

One observation here is that BERT does suspiciously well on unanswerable questions in the news, but not in blogs. This could be due to the fact that news style and vocabulary are quite far from everyday speech, making the crowdworkers' questions about events/entities not in the article easier to detect.

## Partitioning data is hard (but not impossible)

## World knowledge + text-based + unanswerable questions = trouble

Questions that require world knowledge} cannot be answered based on the text alone, but world knowledge makes one of the answer options more likely} (9) cannot be answered with the information in the text, and the world knowledge does not make one of the options more likely.


There's a good reason why people don't combine 

## Paraphrasing hurts

##  



The obvious solution is to aim for more diversity, and to partition the data in ways that would enable more control over what is collected, and how the models are handling it. This is what we attempted to do in QuAIL. In particular, we were interested having a wide selection of questions according to the following criteria:


* **question type balance**: most crowdsourced RC datasets provide the workers with minimal instructions and only analyze a small sample of the generated questions to see what question types they got, or limit such analysis to simple statistics by the first word of the question. Since crowd workers generally aim to perform task as quickly as possible with the least amount of effort, this may lead to the most obvious types of questions over-represented at the expense of others, producing the impression that the models are doing better than they are.
* **question type annotation**: we show that it is possible to generate questions of specific types and collect question type annotations, which enable fine-grained diagnostics of the model performance.
* **open and closed-world questions**: while many RC datasets focus on Wikipedia and news (texts that describe facts likely known from other sources and available in pretraining data of models like BERT), QuAIL's fiction and user stories texts can be assumed to describe unique situations.
* reasoning across the **full range of uncertaingy**: RC datasets typically assume that the answer is to be found or not in the provided documents, and commonsense reasoning datasets assume that the model needs to have some kind of extra knowledge resources it could use. QuAIL is the first dataset to combine text-based, world knowledge and unanswerable questions. In this setting, **the model has to know when it can find an answer, when it could make a confident guess, and when guessing would not be fruitful**.

That was a lot of things to try in just one dataset, but we learned a lot that could be helpful for future RC research. Here are the main points.

# Caveats and future work

The diversity of question types that we aimed for necessitated multi-choice format

Annotator diversity
add adversarial distractors



This choice of question types meant that we have to use multi-choice rather than extractive format. 