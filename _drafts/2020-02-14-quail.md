---
layout: post
slug: quail
author: arogers
title:  "Question Answering for Artificial Intelligence (QuAIL)"
date:   2020-02-14 09:00:47
tags: qa 
mathjax: false
toc: true
excerpt: "QuAIL is a new challenging NLP benchmark that combines reading comprehension and commonsense reasoning."
summary_image: /assets/images/quail-card-square.png 
---

> This blog post summarizes our AAAI 2020 paper "Getting Closer to AI-complete Question Answering: A Set of Prerequisite Real Tasks" {% cite RogersKovalevaEtAl_2020_Getting_Closer_to_AI_Complete_Question_Answering_Set_of_Prerequisite_Real_Tasks %}. Paper PDF: [https://aaai.org/Papers/AAAI/2020GB/AAAI-RogersA.7778.pdf](https://aaai.org/Papers/AAAI/2020GB/AAAI-RogersA.7778.pdf) 

Since 2018 NLP saw an explosion of new verbal reasoning datasets: reading comprehension, commonsense reasoning, natural language inference. However, many such datasets got "solved" almost immediately. The suspicious ease of the datasets prompted concerns about data artifacts and biases, which enable models achieve seemingly super-human accuracy without any real verbal reasoning skills.

One of the top reasons for the datasets being so easy is poor diversity of data that is generated in large amounts by crowd workers with the same kind of prompts and instructions. This may result in large portions of data exhibiting spurious patterns that the model learns to strongly associate with a particular label. For example, the word "never" is strongly associated with the contradiction label in SNLI {% cite GururanganSwayamdiptaEtAl_2018_Annotation_Artifacts_in_Natural_Language_Inference_Data%}, simply because negating was an easy strategy for the crowd workers to generate contradictory statements. If a few such patterns cover large portions of the data, a model may get high accuracy with no linguistic competence.

To address the problem, fundamentally we need to reduce the amount of spurious correlations with predicted labels, which would hopefully force our models to learn generalizable patterns rather than dataset-specific "shortcuts". Currently the community is exploring the following approaches to achieving that:

* adversarial authoring with a model-in-the-loop {% cite DuaWangEtAl_2019_DROP_Reading_Comprehension_Benchmark_Requiring_Discrete_Reasoning_Over_Paragraphs %}
* collections of datasets instead of individual datasets {% cite DuaGottumukkalaEtAl_2019_ORB_Open_Reading_Benchmark_for_Comprehensive_Evaluation_of_Machine_Reading_Comprehension %}, so that the model would be forced to generalize between different kinds of data.

However, both of these approaches have caveats. Combining different datasets means that the questions are specific to particular texts, which gives the model an extra clue about what kinds of reasoning is associated with what inputs (e.g. some questions could be more typical for news than for Wikipedia). Adversarial authoring might fit too closely the hypothesis of a specific model-in-the-loop, which would mean that another model could find some other shortcuts. For instance, the authors of [XXX] report that out of three models the model used as the adversary (BERT) showed the worst performance (although the others suffered too). {% cite BartoloRobertsEtAl_2020_Beat_AI_Investigating_Adversarial_Human_Annotations_for_Reading_Comprehension %} show that stronger adversaries change the question distribution in a way that prevents some weaker models from generalizing to the original distribution. 

QuAIL was developed concurrently with datasets proposing adversarial and multi-dataset approaches, and it explores an alternative solution: partitioning the dataset into balanced subsections written with diverse prompts. This should decrease the likelihood of a single bias affecting a large portion of data. In particular, we have 4x9 design: 4 domains by 9 question types, each with approximately the same number of questions. A crucial point not done in most other datasets is the annotation for both question types and domains, which would enable diagnostics of both the models and the data. 

QuAIL is not as large as SQuAD (15K questions), but we have learned a lot from making it. Here are the basic takeaways.

## Partitioned data is great!

QuAIL is a multi-domain dataset featuring news, blogs, fiction and user stories. Each domain is represented by 200 texts, which gives us a 4-way data split. The texts are CC-licensed and 300-350 words long: excerpts that were hand-picked so as to make sense to human readers without larger context. 

Note that most current SOTA systems are based on large pre-trained models like BERT, which raises the sensitive issue of overlap between pre-training data and the task dataset. In particular, BERT is trained on Wikipedia + BookCorpus, but the authors still tested it on SQuAD {% cite DevlinChangEtAl_2019_BERT_Pre-training_of_Deep_Bidirectional_Transformers_for_Language_Understanding %}. In QuAIL, two of the domains (news and, to some degree, blogs) contain information that is factual and *could* be found in other texts in a large pretraining corpus. However, fiction (by recent and not well-known writers) and personal stories shared on Quora could be assumed to describe unique combinations of events and characters. 

In addition to the domains, we have 9 question types: 

* reasoning about factual information in the text;
* temporal order of events;
* character identity;
* causality;
* properties and qualities of characters;
* belief states; 
* subsequent state after the narrated events;
* duration of the narrated events;
* unanswerable questions

This choice of question types meant that we have to use multi-choice rather than extractive format

This gives us the total of 4 x 9 balanced subsets of the data, which come with annotations that could be used for diagnosing both the models (what they can and can't do) and the data (finding the sections that are suspiciously easy). 

Here are the results we got for a heuristic baseline (longest choice), simple baselines 

<div class="table-wrapper" markdown="block">

| Model      | Temporal order | Entity properties | Causality | Factual | Subsequent state | Event duration | Entity properties | Belief states |        |        |
|------------|----------------|-------------------|-----------|---------|------------------|----------------|-------------------|---------------|--------|--------|
| LongChoice | 36.3           | 32.3              | 46.8      | 35.9    | 29.5             | 33.6           | 35.0              | 30.9          | 12.2   | 35.6   |
| LSTM       | 37.0           | 32.4              | 38.5      | 20.2    | 36.8             | 43.6           | 30.8              | 34.7          | 51.8   | 37.2   |
| PMI        | 42.5           | 48.3              | 57.8      | *57.5*  | 32.9             | 37.0           | 33.7              | 37.5          | 23.3   | 41.8   |
| IR         | 27.9           | 30.0              | 42.5      | 30.8    | 29.6             | 35.4           | 27.5              | 32.0          | 28.8   | 32.4   |
| TriAN      | *55.5*         | *53.1*            | 60.1      | 55.0    | 47.5             | 56.9           | 45.8              | 43.3          | 65.0   | 54.7   |
| BERT       | 52.9           | 46.2              | *67.1*    | 55.8    | *56.7*           | *63.8*         | *48.8*            | *55.0*        | 54.2   | *55.9* |

</div>

<div class="table-wrapper" markdown="block">

| Question types  | Fiction | News  | Blogs | User stories |
|-----------------|---------|-------|-------|--------------|
| Text-based      | 45.5    | 38.8  | 60.5  | *61.6*       |
| World knowledge | *61*    | 58    | 58.3  | 55.6         |
| Unanswerable    | 58.3    | *68.3*| 40    | 50           |
| All questions   | 55.5    | 52.7  | *57*  | *57*         |

</div>

## Partitioning data is hard (but not impossible)

## World knowledge + text-based + unanswerable questions = trouble

Questions that require world knowledge} cannot be answered based on the text alone, but world knowledge makes one of the answer options more likely} (9) cannot be answered with the information in the text, and the world knowledge does not make one of the options more likely.


There's a good reason why people don't combine 

## Paraphrasing hurts

##  



The obvious solution is to aim for more diversity, and to partition the data in ways that would enable more control over what is collected, and how the models are handling it. This is what we attempted to do in QuAIL. In particular, we were interested having a wide selection of questions according to the following criteria:


* **question type balance**: most crowdsourced RC datasets provide the workers with minimal instructions and only analyze a small sample of the generated questions to see what question types they got, or limit such analysis to simple statistics by the first word of the question. Since crowd workers generally aim to perform task as quickly as possible with the least amount of effort, this may lead to the most obvious types of questions over-represented at the expense of others, producing the impression that the models are doing better than they are.
* **question type annotation**: we show that it is possible to generate questions of specific types and collect question type annotations, which enable fine-grained diagnostics of the model performance.
* **open and closed-world questions**: while many RC datasets focus on Wikipedia and news (texts that describe facts likely known from other sources and available in pretraining data of models like BERT), QuAIL's fiction and user stories texts can be assumed to describe unique situations.
* reasoning across the **full range of uncertaingy**: RC datasets typically assume that the answer is to be found or not in the provided documents, and commonsense reasoning datasets assume that the model needs to have some kind of extra knowledge resources it could use. QuAIL is the first dataset to combine text-based, world knowledge and unanswerable questions. In this setting, **the model has to know when it can find an answer, when it could make a confident guess, and when guessing would not be fruitful**.

That was a lot of things to try in just one dataset, but we learned a lot that could be helpful for future RC research. Here are the main points.

# Caveats and future work

The diversity of question types that we aimed for necessitated multi-choice format

