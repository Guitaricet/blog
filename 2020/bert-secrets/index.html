<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset='utf-8'>
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    <!--link rel="stylesheet" href="/_sass/jekyll-theme-architect.scss" media="screen" type="text/css">
    <link rel="stylesheet" href="/_sass/normalize.scss" media="screen" type="text/css">
    <link rel="stylesheet" href="/_sass/rouge-github.scss" media="screen" type="text/css"-->
    <link rel="stylesheet" href="/blog/assets/css/style.css?v=" media="screen" type="text/css">
    <link rel="stylesheet" href="/blog/assets/css/print.css" media="print" type="text/css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.6.3/css/font-awesome.min.css" type="text/css">

    <!--[if lt IE 9]-->
    <script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>

<!-- Default Statcounter code for Text-machine Blog
https://text-machine-lab.github.io/blog/ -->
<script type="text/javascript">
var sc_project=12176921;
var sc_invisible=1;
var sc_security="e9a6e2dd";
</script>
<script type="text/javascript"
src="https://www.statcounter.com/counter/counter.js"
async></script>
<noscript><div class="statcounter"><a title="Web Analytics"
href="https://statcounter.com/" target="_blank"><img
class="statcounter"
src="https://c.statcounter.com/12176921/0/e9a6e2dd/1/"
alt="Web Analytics"></a></div></noscript>
<!-- End of Statcounter Code -->

<!-- Twitter metadata -->
<meta name="twitter:card" content="">
<meta name="twitter:site" content="">
<meta name="twitter:title" content="The Dark Secrets of BERT">

  <meta name="twitter:description"
    content="">

<!-- end of Twitter metadata -->

<meta name="og:image" content="/blog/assets/images/text-machine-logo-transparent.png">

<!-- Begin Jekyll SEO tag v2.6.1 -->
<title>The Dark Secrets of BERT | Text Machine Blog</title>
<meta name="generator" content="Jekyll v3.7.4" />
<meta property="og:title" content="The Dark Secrets of BERT" />
<meta name="author" content="Anna Rogers" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="BERT and its Transformer-based cousins are still ahead on all NLP leaderboards. But how much do they actually understand about language?" />
<meta property="og:description" content="BERT and its Transformer-based cousins are still ahead on all NLP leaderboards. But how much do they actually understand about language?" />
<link rel="canonical" href="https://text-machine-lab.github.io/blog/2020/bert-secrets/" />
<meta property="og:url" content="https://text-machine-lab.github.io/blog/2020/bert-secrets/" />
<meta property="og:site_name" content="Text Machine Blog" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-01-07T16:00:47-05:00" />
<script type="application/ld+json">
{"description":"BERT and its Transformer-based cousins are still ahead on all NLP leaderboards. But how much do they actually understand about language?","author":{"@type":"Person","name":"Anna Rogers"},"@type":"BlogPosting","url":"https://text-machine-lab.github.io/blog/2020/bert-secrets/","headline":"The Dark Secrets of BERT","dateModified":"2020-01-07T16:00:47-05:00","datePublished":"2020-01-07T16:00:47-05:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://text-machine-lab.github.io/blog/2020/bert-secrets/"},"@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

  </head>

  <body>
    <header>
        <div class="nav-menu">
            <ul>
              <li><a href="/blog/tags"><i class="fa fa-hashtag"></i> Tag index</a></li>
              <li><a href="/blog/years"><i class="fa fa-list"></i> All posts</a></li>
              <li><a href="https://github.com/text-machine-lab/"> <i class="fa fa-github"></i> GitHub</a></li>
              <li><a href="http://text-machine.cs.uml.edu/"><i class="fa fa-home"></i> About us</a></li>
            </ul>
        </div>

      <div class="inner">
          <div>
            <a href="http://text-machine.cs.uml.edu/"><img src="/blog/assets/images/text-machine-logo-transparent.png" alt="Text Machine logo" class="logo"></a>
          </div>
        <div>
        <a href="https://text-machine-lab.github.io/blog/">
          <h1>Text Machine Blog</h1>
        </a>
        <h2 class="tagline">Machine Learning, NLP, and more</h2>
        </div>
      </div>

    </header>

    <div id="content-wrapper">
      <div class="inner clearfix">
        <section id="main-content">
          

<h1>The Dark Secrets of BERT</h1>

<span class="post-date">07 Jan 2020 • </span>
<img src="/blog/assets/images/time-button.jpg" class="read-time"></img>
<span class="reading-time" title="Estimated read time">
  
  
    14 mins
  
</span>

<p><strong>Tags:</strong>
  <span>
  
    
    <a href="/tag/transformers"><code class="highligher-rouge"><nobr>transformers</nobr></code>&nbsp;</a>
  
   </span>

</p>



<!--p class="post-author">Author: Anna Rogers</p-->
  
      <hr>
<span>

<div class="author-container">
        
            <img src="/blog/assets/images/aro.jpg" class="author-img">
        
    <div class="author-text">
        <span class="author-name"> Anna Rogers &nbsp;</span>
           
              <a href="http://www.cs.uml.edu/~arogers/"><span class="label"><i class="fa fa-home"></i> Profile</span></a>
           
           
              <a href="http://twitter.com/annargrs"><span class="label"><i class="fa fa-twitter"></i> Twitter</span></a>
            
           
              <a href="http://hackingsemantics.xyz/" > <span class="label"> <i class="fa fa-pencil"></i> Blog</span></a>
           
           
           <br/> <i class="fa id-badge"></i> <i>Anna Rogers is a computational linguist working on meaning representations for NLP, social NLP, and question answering. She was a post-doctoral associate in the Text Machine Lab in 2017-2019.</i>
           
    </div>
</div>


  

<blockquote>
  <p>This blog post summarizes our EMNLP 2019 paper “Revealing the Dark Secrets of BERT” <a class="citation" href="#KovalevaRomanovEtAl_2019_Revealing_Dark_Secrets_of_BERT">(Kovaleva, Romanov, Rogers, &amp; Rumshisky, 2019)</a>. Paper PDF: <a href="https://www.aclweb.org/anthology/D19-1445.pdf">https://www.aclweb.org/anthology/D19-1445.pdf</a></p>
</blockquote>

<p>2019 could be called the year of the Transformer in NLP: this architecture <a href="https://hackingsemantics.xyz/2019/leaderboards/">dominated the leaderboards</a> and inspired many analysis studies. The most popular Transformer is, undoubtedly, BERT <a class="citation" href="#DevlinChangEtAl_2019_BERT_Pre-training_of_Deep_Bidirectional_Transformers_for_Language_Understanding">(Devlin, Chang, Lee, &amp; Toutanova, 2019)</a>. In addition to its numerous applications, multiple studies probed this model for various kinds of linguistic knowledge, typically to conclude that such knowledge is indeed present, to at least some extent <a class="citation" href="#Goldberg_2019_Assessing_BERTs_Syntactic_Abilities">(Goldberg, 2019; Hewitt &amp; Manning, 2019; Ettinger, 2019)</a>.</p>

<p>This work focuses on the complementary question: what happens in the <em>fine-tuned</em> BERT? In particular, how much of the linguistically interpretable self-attention patterns that are presumed to be its strength are actually used to solve downstream tasks?</p>

<p>To answer this question, we experiment with BERT fine-tuned on the following GLUE <a class="citation" href="#WangSinghEtAl_2018_GLUE_A_Multi-Task_Benchmark_and_Analysis_Platform_for_Natural_Language_Understanding">(Wang et al., 2018)</a> tasks and datasets:</p>

<ul>
  <li>paraphrase detection (MRPC and QQP);</li>
  <li>textual similarity (STS-B);</li>
  <li>sentiment analysis (SST-2);</li>
  <li>textual entailment (RTE);</li>
  <li>natural language inference (QNLI, MNLI).</li>
</ul>

<h2 id="a-brief-intro-to-bert">A brief intro to BERT</h2>

<p>BERT stands for Bidirectional Encoder Representations from Transformers. This model is basically a multi-layer bidirectional Transformer encoder <a class="citation" href="#DevlinChangEtAl_2019_BERT_Pre-training_of_Deep_Bidirectional_Transformers_for_Language_Understanding">(Devlin, Chang, Lee, &amp; Toutanova, 2019)</a>, and there are multiple excellent guides about how it works generally, including <a href="https://jalammar.github.io/illustrated-transformer/">the Illustrated Transformer</a>. What we focus on is one specific component of Transformer architecture known as self-attention. In a nutshell, it is a way to weigh the components of the input and output sequences so as to model relations between them, even long-distance dependencies.</p>

<p>As a brief example, let’s say we need to create a representation of the sentence “Tom is a black cat”. BERT may choose to pay more attention to “Tom” while encoding the word “cat”, and less attention to the words “is”, “a”, “black”. This could be represented as a vector of weights (for each word in the sentence). Such vectors are computed when the model encodes each word in the sequence, yielding a square matrix which we refer to as the self-attention map.</p>

<p>Now, a priori it is not clear that the relation between “Tom” and “cat” is always the best one. To answer questions about the color of the cat, a model would do better to focus on “black” rather than “Tom”. Luckily, it doesn’t have to choose. The power of BERT (and other Transformers) is largely attributed to the fact that there are multiple heads in multiple layers that all learn to construct independent self-attention maps. Theoretically, this could give the model the capacity to “attend to information from different representation subspaces at different positions” <a class="citation" href="#VaswaniShazeerEtAl_2017_Attention_is_all_you_need">(Vaswani et al., 2017)</a>. In other words, the model would be able to choose between several alternative representations for the task at hand.</p>

<p>Most of the computation of self-attention weights happens in BERT during pre-training: the model is (pre-)trained on two tasks (masked language model and next sentence prediction), and subsequently fine-tuned for individual downstream tasks such as sentiment analysis. The basic idea for this separation of the training process into semi-supervised pre-training and supervised fine-tuning phases is that of transfer learning: the task datasets are typically too small to learn enough about language in general, but large text corpora can be used for this via the language modeling objective (and other similar ones). We could thus get task-independent, but informative representations of sentences and texts, which then could be “adapted” for the downstream tasks.</p>

<p>Let us note here that the exact way the “adaptation” is supposed to work is not described in detail in either BERT paper or the GPT technical report (which highlighted the pre-training/fine-tuning approach). However, if attention itself is meant to provide a way to “link” parts of the input sequence so as to increase its informativeness, and the multi-head, multi-layer architecture is needed to provide multiple alternative attention maps, presumably the fine-tuning process would teach the model to rely on the maps that are more useful for the task at hand. For instance, one could expect that relations between nouns and adjectives are more important for sentiment analysis task than relations between nouns and prepositions, and so fine-tuning would ideally teach the model to rely more on the more useful self-attention maps.</p>

<h2 id="what-types-of-self-attention-patterns-are-learned-and-how-many-of-each-type">What types of self-attention patterns are learned, and how many of each type?</h2>

<p>So what are the patterns of the self-attention in BERT? We found five, as shown below:</p>

<figure>
	<img src="/blog/assets/images/bert-attn-types.png" /> 
	<figcaption>Fig. 1. Types of self-attention patterns in BERT. Both axes on every image represent BERT tokens of an input example, and colors denote absolute attention weights (darker colors stand for greater weights).
	</figcaption>
</figure>

<ul>
  <li>The vertical pattern indicates attention to a single token, which usually is either the [SEP] token (special token representing the end of a sentence), or [CLS] (special BERT token that is used as full sequence representation fed to the classifiers).</li>
  <li>The diagonal pattern indicates the attention to previous/next words;</li>
  <li>The block pattern indicates more-or-less uniform attention to all tokens in a sequence;</li>
  <li>The heterogeneous pattern is the only pattern that theoretically <em>could</em> correspond to anything like meaningful relations between parts of the input sequence (although not necessarily so).</li>
</ul>

<p>And here are the ratios of these five types of attention in BERT fine-tuned on seven GLUE tasks (with each column representing 100% of all heads in all layers):</p>

<figure>
	<img src="/blog/assets/images/bert-attn-ratios.png" /> 	
	<figcaption>Fig. 2. Ratios of self-attention map types for BERT fine-tuned on the selected GLUE tasks.
	</figcaption>
</figure>

<p>While the exact ratios vary by the task, in most cases the potentially meaningful patterns constitute less than half of all BERT self-attention weights. At least a third of BERT heads attends simply to [SEP] and [CLS] tokens - a strategy that cannot contribute much of meaningful information to the next layer’s representations. It also shows that the model is severely overparametrized, which explains the recent successful attempts of its distillation <a class="citation" href="#SanhDebutEtAl_2019_DistilBERT_distilled_version_of_BERT_smaller_faster_cheaper_and_lighter">(Sanh, Debut, Chaumond, &amp; Wolf, 2019; Jiao et al., 2019)</a>.</p>

<p>Note that we experimented with BERT-base, the smaller model with 12 heads in 16 layers. If it is already so overparametrized, this has implications for BERT-large and all the later models, some of which are 30 times larger <a class="citation" href="#WuSchusterEtAl_2016_Googles_neural_machine_translation_system_bridging_gap_between_human_and_machine_translation">(Wu et al., 2016)</a>.</p>

<p>Such reliance on [SEP] and [CLS] tokens could also suggest that either they somehow “absorb” the informative representations obtained in the earlier layers, and subsequent self-attention maps are simply not needed much, or that BERT overall does not rely on self-attention maps to the degree to which one would expect for this key component of this architecture.</p>

<h2 id="what-happens-in-fine-tuning">What happens in fine-tuning?</h2>

<p>Our next question was what actually changes during the fine-tuning of BERT. The heatmap below shows the cosine similarities between flattened self-attention map matrices in each head and each layer, before and after fine-tuning. Darker colors indicate more differences in the representation. For all GLUE tasks the fine-tuning was done for 3 epochs.</p>

<figure>
	<img src="/blog/assets/images/bert-finetune-diff.png" /> 		
	<figcaption>Fig. 3. Cosine similarity between flattened self-attention maps, per head in pre-trained and fine-tuned BERT. Darker colors indicate greater differences.
	</figcaption>
</figure>

<p>We see that most attention weights do not change all that much, and for most tasks, the last two layers show the most change. These changes do not appear to favor any specific types of meaningful attention patterns. Instead, we find that the model basically learns to rely more on the vertical attention pattern. In the SST example below the thicker vertical attention patterns in the last layers are due to the joint attention to the final [SEP] and the punctuation tokens preceding it, which we observed to be another frequent target for the vertical attention pattern.</p>

<figure>
	<img src="/blog/assets/images/bert-sst-heads.png" /> 			
	<figcaption>Fig. 4. Self-attention maps for an individual example, with BERT fine-tuned on SST.
	</figcaption>
</figure>

<p>This has two possible explanations:</p>

<ul>
  <li>the vertical pattern is somehow sufficient, i.e. the [SEP] token representations somehow absorbed the meaningful attention patterns from previous layers. We did find that the earliest layers attend to [CLS] more, and then [SEP] starts dominating for most tasks (see Fig. 6 in the paper);</li>
  <li>the tasks at hand do not actually require the fine-grained meaningful attention patterns that are supposed to be the main feature of the Transformers.</li>
</ul>

<h2 id="how-much-difference-does-fine-tuning-make">How much difference does fine-tuning make?</h2>

<p>Given the vast discrepancies in the size of the datasets used in pre-training and fine-tuning, and the very different training objectives, it is interesting to investigate how much of a difference fine-tuning actually makes. To the best of our knowledge, this question has not been addressed before.</p>

<p>We conduct three experiments on each of the selected GLUE datasets:</p>

<ul>
  <li>BERT performance with weights frozen from pre-training and fed to the task-specific classifiers;</li>
  <li>BERT performance with a model randomly initialized from normal distribution, and fine-tuned on task datasets for 3 epoches;</li>
  <li>BERT performance with the official pretrained BERT-base model, fine-tuned on task datasets for 3 epochs.</li>
</ul>

<p>The results of this experiment were as follows:</p>

<div class="table-wrapper">

  <table>
    <thead>
      <tr>
        <th>Dataset</th>
        <th>Pretrained</th>
        <th>Random+finetuned</th>
        <th>Pretrained+finetuned</th>
        <th>Metric</th>
        <th>Dataset size</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>MRPC</td>
        <td>31.6</td>
        <td>68.3</td>
        <td>82.3</td>
        <td>Acc</td>
        <td>5.8K</td>
      </tr>
      <tr>
        <td>STS-B</td>
        <td>33.1</td>
        <td>2.9</td>
        <td>82.7</td>
        <td>Acc</td>
        <td>8.6K</td>
      </tr>
      <tr>
        <td>SST-2</td>
        <td>49.1</td>
        <td>80.5</td>
        <td>92</td>
        <td>Acc</td>
        <td>70K</td>
      </tr>
      <tr>
        <td>QQP</td>
        <td>60.9</td>
        <td>63.2</td>
        <td>78.6</td>
        <td>Acc</td>
        <td>400K</td>
      </tr>
      <tr>
        <td>RTE</td>
        <td>52.7</td>
        <td>52.7</td>
        <td>64.6</td>
        <td>Acc</td>
        <td>2.7K</td>
      </tr>
      <tr>
        <td>QNLI</td>
        <td>52.8</td>
        <td>49.5</td>
        <td>84.4</td>
        <td>Acc</td>
        <td>130K</td>
      </tr>
      <tr>
        <td>MNLI-m</td>
        <td>31.7</td>
        <td>61.0</td>
        <td>78.6</td>
        <td>Acc</td>
        <td>440K</td>
      </tr>
    </tbody>
  </table>

</div>

<p>While it is clear that pretraining + fine-tuning setup yields the highest results, the <strong>random + fine-tuned BERT is doing disturbingly well on all tasks except textual similarity</strong>. Indeed, for sentiment analysis it appears that one could get 80% accuracy with randomly initialized and fine-tuned BERT, without any pre-training. Given the scale of the large pre-trained Transformers, this raises serious questions about whether the expensive pre-training yields enough bang for the buck. It also raises serious questions about the NLP datasets that apparently can be solved without much task-independent linguistic knowledge that the pre-training + fine-tuning setup was supposed to deliver.</p>

<h2 id="are-there-any-linguistically-interpretable-self-attention-heads">Are there any linguistically interpretable self-attention heads?</h2>

<p>Several studies at this point tried to locate self-attention heads that encode specific types of information, but most of them focused on syntax. We conducted an experiment focusing on frame semantic elements: we extracted from FrameNet 1.7 <a class="citation" href="#BakerFillmoreEtAl_1998_Berkeley_Framenet_project">(Baker, Fillmore, &amp; Lowe, 1998)</a> 473 sentences which were at most 12 tokens in length (to reduce the number of sentences evoking multiple frames), and which had a core frame element at a distance of at least 2 tokens from the target word (irrespective of its syntactic function). In the example below, it is the relation between the Experiencer and the participle “agitated” that evokes the Emotion_directed frame. Arguably, such relations are indeed core to understanding the situation described by a given sentence, and any mechanism claiming to provide linguistically informative self-attention maps should reflect them (possibly among many other relations).</p>

<p>We obtained representations of these sentences by pre-trained BERT, calculating the maximum weights between token pairs corresponding to the annotated frame semantic relations. Fig. 5 represents the averages of these scores for all examples in our FrameNet dataset. We found two heads (head 2 in layer 1, head 6 in layer 7) that attended to these frame semantic relations more than the other heads.</p>

<figure>
	<img src="/blog/assets/images/bert-frames.png" /> 				
	<figcaption>Fig. 5. The heads of pre-trained BERT that appear to encode the information correlated to semantic links in the input text.
	</figcaption>
</figure>

<h2 id="but-what-information-actually-gets-used-at-inference-time">But… what information actually gets used at inference time?</h2>

<p>We believe it would be too rash to conclude from probes of pre-trained BERT weights that certain information is actually encoded. Given the size of the model, it might be possible to find similar proof of encoding for any other relation (and the fact that Jawahar et al. found no significant difference between different decomposition schemes points in that direction <a class="citation" href="#JawaharSagotEtAl_2019_What_does_BERT_learn_about_structure_of_language">(Jawahar, Sagot, &amp; Seddah, 2019)</a>). The real question is whether the model learns to actually rely on that information at inference time.</p>

<p>To see whether the two heads we identified as useful for encoding frame semantic relations actually get used by fine-tuned BERT, we performed an ablation study, disabling one head at a time (i.e. replacing the learned attention weights with uniform attention). Fig. 6 shows a heatmap for all GLUE tasks in our sample, with each cell indicating the overall performance when a given head was switched off. It is clear that while the overall pattern varies between tasks, on average we are better off removing a random head - including those that we identified as encoding meaningful information that should be relevant for most tasks. Many of the heads can also be switched off without any effect on performance, again pointing at the fact that even the base BERT is severely overparametrized.</p>

<figure>
	<img src="/blog/assets/images/bert-ablate-heads.png" /> 			
	<figcaption>Fig. 6. Performance of the model while disabling one head at a time. The orange line indicates the baseline performance with no disabled heads. Darker colors correspond to greater performance scores.
	</figcaption>
</figure>

<p>Similar conclusions were reached independently for machine translation task, with zeroing attention weights rather than replacing them with uniform attention <a class="citation" href="#MichelLevyEtAl_2019_Are_Sixteen_Heads_Really_Better_than_One">(Michel, Levy, &amp; Neubig, 2019)</a>. We further show that this observations extends not to just heads, but whole layers: depending on the task, a whole layer may be detrimental to the model performance!</p>

<figure>
	<img src="/blog/assets/images/bert-ablate-layers.png" /> 			
	<figcaption>Fig. 7. Performance of the model while disabling one layer at a time.
	</figcaption>
</figure>

<h2 id="so-how-does-this-thing-work">So how does this thing work?</h2>

<p>To sum up, this work showed that even base BERT is severely overparametrized, which explains why model distillation turned out to be so productive.</p>

<p>Our key contribution is that while most studies of BERT focused on probing the pre-trained model, we raised the question of what happens in fine-tuning, and just how meaningful the representations obtained with the self-attention mechanism are. We were unable to find evidence of linguistically meaningful self-attention maps being crucial for the performance of fine-tuned BERT.</p>

<p>These results could be interpreted in the following ways:</p>

<p>a) <strong>BERT is overparametrized</strong>: Since we switch off only one head at a time, it may be possible that some heads are functional duplicates, and removing one head would not harm the model because the same information is available elsewhere. That would again point at overparametrization and importance of model distillation: with a large model, it is not feasible to test this hypothesis by switching off all possible combinations of heads. Promising results for the base Transformer were reported in a contemporaneous study, which identified the “important” heads by fine-tuning the model with a regularized objective that had the pruning effect (missing reference).</p>

<p>b) <strong>BERT’s success is due to <del>black magic</del> something other than self-attention maps</strong>: The information we as humans deem important for solving a verbal reasoning task may genuinely be not needed by the model, as it performs some deeper reasoning we are not able to comprehend or interpret (perhaps relying on some other component than the interpretable self-attention maps). Our results also contribute to the ongoing discussion about the value of attention maps for explaining model predictions (missing reference).</p>

<p>c) <strong>BERT does not need to be all that smart for these tasks</strong>: The model does not actually solve the verbal reasoning task, but learns to rely on various shortcuts, biases and artifacts in the datasets to arrive at the correct prediction, and therefore does not need the attention maps to be particularly informative.</p>

<p>Sll three of the above factors are probably playing a role. However, given our findings about how well a randomly initialized BERT does on most GLUE tasks, and the recent discoveries of problems with many current datasets <a class="citation" href="#GururanganSwayamdiptaEtAl_2018_Annotation_Artifacts_in_Natural_Language_Inference_Data">(Gururangan et al., 2018; McCoy, Pavlick, &amp; Linzen, 2019)</a>, the easy dataset factor seems to be very likely.</p>



<!-- AddToAny BEGIN -->
<div class="a2a_kit a2a_kit_size_32 a2a_default_style">
<a class="a2a_button_twitter"></a>
<a class="a2a_button_reddit"></a>
<a class="a2a_button_facebook"></a>
<a class="a2a_button_telegram"></a>
<a class="a2a_button_hacker_news"></a>
<a class="a2a_button_email"></a>
<a class="a2a_dd" href="https://www.addtoany.com/share"></a>

<!-- LikeBtn.com BEGIN -->
<span class="likebtn-wrapper" data-theme="custom" data-btn_size="40" data-f_size="14" data-icon_size="30" data-icon_l_c="#159031" data-icon_l_c_v="#1405fb" data-icon_d_c="#f40d20" data-icon_d_c_v="#1405fb" data-identifier="item_1" data-show_like_label="false" data-counter_frmt="km"></span>
<script>(function(d,e,s){if(d.getElementById("likebtn_wjs"))return;a=d.createElement(e);m=d.getElementsByTagName(e)[0];a.async=1;a.id="likebtn_wjs";a.src=s;m.parentNode.insertBefore(a, m)})(document,"script","//w.likebtn.com/js/w/widget.js");</script>
<!-- LikeBtn.com END -->

</div>
<script async src="https://static.addtoany.com/menu/page.js"></script>
<!-- AddToAny END -->


<script src="https://utteranc.es/client.js"
        repo="text-machine-lab/blog"
        issue-term="title"
        label="Comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script>

<h2> References </h2>

<ol class="bibliography"><li><div class="text-justify">
    <span id="WuSchusterEtAl_2016_Googles_neural_machine_translation_system_bridging_gap_between_human_and_machine_translation">Wu, Y., Schuster, M., Chen, Z., Le, Q. V., Norouzi, M., Macherey, W., … Dean, J. (2016). Google’s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation. <i>CoRR</i>, <i>abs/1609.08144</i>.</span>

    
    

    <!--button class="btn--info" onclick="showBibtex('WuSchusterEtAl_2016_Googles_neural')">BibTex</button-->
    
    <a href="http://arxiv.org/abs/1609.08144">[PDF]</a>
    
<!--div class="bibtex" id='WuSchusterEtAl_2016_Googles_neural'><pre>@article{WuSchusterEtAl_2016_Googles_neural_machine_translation_system_bridging_gap_between_human_and_machine_translation,
  title = {Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation},
  volume = {abs/1609.08144},
  journal = {CoRR},
  url = {http://arxiv.org/abs/1609.08144},
  author = {Wu, Yonghui and Schuster, Mike and Chen, Zhifeng and Le, Quoc V. and Norouzi, Mohammad and Macherey, Wolfgang and Krikun, Maxim and Cao, Yuan and Gao, Qin and Macherey, Klaus and Klingner, Jeff and Shah, Apurva and Johnson, Melvin and Liu, Xiaobing and Kaiser, {\L}ukasz and Gouws, Stephan and Kato, Yoshikiyo and Kudo, Taku and Kazawa, Hideto and Stevens, Keith and Kurian, George and Patil, Nishant and Wang, Wei and Young, Cliff and Smith, Jason and Riesa, Jason and Rudnick, Alex and Vinyals, Oriol and Corrado, Greg and Hughes, Macduff and Dean, Jeffrey},
  year = {2016}
}
</pre>
http://arxiv.org/abs/1609.08144
</div-->
</div>



<div>
    
</div></li>
<li><div class="text-justify">
    <span id="WangSinghEtAl_2018_GLUE_A_Multi-Task_Benchmark_and_Analysis_Platform_for_Natural_Language_Understanding">Wang, A., Singh, A., Michael, J., Hill, F., Levy, O., &amp; Bowman, S. R. (2018). GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding. <i>Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP</i>, 353–355. Brussels, Belgium: Association for Computational Linguistics.</span>

    
    

    <!--button class="btn--info" onclick="showBibtex('WangSinghEtAl_2018_GLUE_A')">BibTex</button-->
    
    <a href="http://aclweb.org/anthology/W18-5446">[PDF]</a>
    
<!--div class="bibtex" id='WangSinghEtAl_2018_GLUE_A'><pre>@inproceedings{WangSinghEtAl_2018_GLUE_A_Multi-Task_Benchmark_and_Analysis_Platform_for_Natural_Language_Understanding,
  archiveprefix = {arXiv},
  address = {{Brussels, Belgium}},
  title = {{{GLUE}}: {{A Multi}}-{{Task Benchmark}} and {{Analysis Platform}} for {{Natural Language Understanding}}},
  shorttitle = {{{GLUE}}},
  booktitle = {Proceedings of the 2018 {{EMNLP Workshop BlackboxNLP}}: {{Analyzing}} and {{Interpreting Neural Networks}} for {{NLP}}},
  publisher = {{Association for Computational Linguistics}},
  url = {http://aclweb.org/anthology/W18-5446},
  author = {Wang, Alex and Singh, Amapreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R.},
  year = {2018},
  pages = {353--355}
}
</pre>
http://aclweb.org/anthology/W18-5446
</div-->
</div>



<div>
    
</div></li>
<li><div class="text-justify">
    <span id="VaswaniShazeerEtAl_2017_Attention_is_all_you_need">Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., … Polosukhin, I. (2017). Attention Is All You Need. <i>NIPS</i>, 5998–6008. Long Beach, CA, USA.</span>

    
    

    <!--button class="btn--info" onclick="showBibtex('VaswaniShazeerEtAl_2017_Attention_is')">BibTex</button-->
    
    <a href="http://papers.nips.cc/paper/7181-attention-is-all-you-need">[PDF]</a>
    
<!--div class="bibtex" id='VaswaniShazeerEtAl_2017_Attention_is'><pre>@inproceedings{VaswaniShazeerEtAl_2017_Attention_is_all_you_need,
  address = {{Long Beach, CA, USA}},
  title = {Attention Is All You Need},
  booktitle = {{{NIPS}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, \textbackslash{}Lukasz and Polosukhin, Illia},
  year = {2017},
  pages = {5998--6008},
  url = {http://papers.nips.cc/paper/7181-attention-is-all-you-need}
}
</pre>
http://papers.nips.cc/paper/7181-attention-is-all-you-need
</div-->
</div>



<div>
    
</div></li>
<li><div class="text-justify">
    <span id="SanhDebutEtAl_2019_DistilBERT_distilled_version_of_BERT_smaller_faster_cheaper_and_lighter">Sanh, V., Debut, L., Chaumond, J., &amp; Wolf, T. (2019). DistilBERT, a Distilled Version of BERT: Smaller, Faster, Cheaper and Lighter. <i>5th Workshop on Energy Efficient Machine Learning and Cognitive Computing - NeurIPS 2019</i>.</span>

    
    

    <!--button class="btn--info" onclick="showBibtex('SanhDebutEtAl_2019_DistilBERT_distilled')">BibTex</button-->
    
    <a href="http://arxiv.org/abs/1910.01108">[PDF]</a>
    
<!--div class="bibtex" id='SanhDebutEtAl_2019_DistilBERT_distilled'><pre>@inproceedings{SanhDebutEtAl_2019_DistilBERT_distilled_version_of_BERT_smaller_faster_cheaper_and_lighter,
  archiveprefix = {arXiv},
  title = {{{DistilBERT}}, a Distilled Version of {{BERT}}: Smaller, Faster, Cheaper and Lighter},
  shorttitle = {{{DistilBERT}}, a Distilled Version of {{BERT}}},
  booktitle = {5th {{Workshop}} on {{Energy Efficient Machine Learning}} and {{Cognitive Computing}} - {{NeurIPS}} 2019},
  url = {http://arxiv.org/abs/1910.01108},
  author = {Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},
  year = {2019}
}
</pre>
http://arxiv.org/abs/1910.01108
</div-->
</div>



<div>
    
</div></li>
<li><div class="text-justify">
    <span id="MichelLevyEtAl_2019_Are_Sixteen_Heads_Really_Better_than_One">Michel, P., Levy, O., &amp; Neubig, G. (2019). Are Sixteen Heads Really Better than One? <i>NeurIPS</i>.</span>

    
    

    <!--button class="btn--info" onclick="showBibtex('MichelLevyEtAl_2019_Are_Sixteen')">BibTex</button-->
    
    <a href="http://arxiv.org/abs/1905.10650">[PDF]</a>
    
<!--div class="bibtex" id='MichelLevyEtAl_2019_Are_Sixteen'><pre>@article{MichelLevyEtAl_2019_Are_Sixteen_Heads_Really_Better_than_One,
  archiveprefix = {arXiv},
  title = {Are {{Sixteen Heads Really Better}} than {{One}}?},
  journal = {NeurIPS},
  url = {http://arxiv.org/abs/1905.10650},
  author = {Michel, Paul and Levy, Omer and Neubig, Graham},
  year = {2019},
  ids = {MichelLevyEtAl\_2019\_Are\_Sixteen\_Heads\_Really\_Better\_than\_Onea}
}
</pre>
http://arxiv.org/abs/1905.10650
</div-->
</div>



<div>
    
</div></li>
<li><div class="text-justify">
    <span id="McCoyPavlickEtAl_2019_Right_for_Wrong_Reasons_Diagnosing_Syntactic_Heuristics_in_Natural_Language_Inference">McCoy, T., Pavlick, E., &amp; Linzen, T. (2019). Right for the Wrong Reasons: Diagnosing Syntactic Heuristics in Natural Language Inference. <i>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</i>, 3428–3448. Florence, Italy: Association for Computational Linguistics.</span>

    
    

    <!--button class="btn--info" onclick="showBibtex('McCoyPavlickEtAl_2019_Right_for')">BibTex</button-->
    
    <a href="https://www.aclweb.org/anthology/P19-1334/">[PDF]</a>
    
<!--div class="bibtex" id='McCoyPavlickEtAl_2019_Right_for'><pre>@inproceedings{McCoyPavlickEtAl_2019_Right_for_Wrong_Reasons_Diagnosing_Syntactic_Heuristics_in_Natural_Language_Inference,
  address = {{Florence, Italy}},
  title = {Right for the {{Wrong Reasons}}: {{Diagnosing Syntactic Heuristics}} in {{Natural Language Inference}}},
  shorttitle = {Right for the {{Wrong Reasons}}},
  booktitle = {Proceedings of the 57th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  publisher = {{Association for Computational Linguistics}},
  url = {https://www.aclweb.org/anthology/P19-1334/},
  author = {McCoy, Tom and Pavlick, Ellie and Linzen, Tal},
  year = {2019},
  pages = {3428--3448}
}
</pre>
https://www.aclweb.org/anthology/P19-1334/
</div-->
</div>



<div>
    
</div></li>
<li><div class="text-justify">
    <span id="KovalevaRomanovEtAl_2019_Revealing_Dark_Secrets_of_BERT">Kovaleva, O., Romanov, A., Rogers, A., &amp; Rumshisky, A. (2019). Revealing the Dark Secrets of BERT. <i>Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</i>, 4356–4365. https://doi.org/10.18653/v1/D19-1445</span>

    
    

    <!--button class="btn--info" onclick="showBibtex('KovalevaRomanovEtAl_2019_Revealing_Dark')">BibTex</button-->
    
    <a href="https://www.aclweb.org/anthology/D19-1445">[PDF]</a>
    
<!--div class="bibtex" id='KovalevaRomanovEtAl_2019_Revealing_Dark'><pre>@inproceedings{KovalevaRomanovEtAl_2019_Revealing_Dark_Secrets_of_BERT,
  address = {{Hong Kong, China}},
  title = {Revealing the {{Dark Secrets}} of {{BERT}}},
  booktitle = {Proceedings of the 2019 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} and the 9th {{International Joint Conference}} on {{Natural Language Processing}} ({{EMNLP}}-{{IJCNLP}})},
  publisher = {{Association for Computational Linguistics}},
  doi = {10.18653/v1/D19-1445},
  url = {https://www.aclweb.org/anthology/D19-1445},
  author = {Kovaleva, Olga and Romanov, Alexey and Rogers, Anna and Rumshisky, Anna},
  year = {2019},
  pages = {4356--4365}
}
</pre>
https://www.aclweb.org/anthology/D19-1445
</div-->
</div>



<div>
    
</div></li>
<li><div class="text-justify">
    <span id="JiaoYinEtAl_2019_TinyBERT_Distilling_BERT_for_Natural_Language_Understanding">Jiao, X., Yin, Y., Shang, L., Jiang, X., Chen, X., Li, L., … Liu, Q. (2019). TinyBERT: Distilling BERT for Natural Language Understanding. <i>ArXiv Preprint ArXiv:1909.10351</i>.</span>

    
    

    <!--button class="btn--info" onclick="showBibtex('JiaoYinEtAl_2019_TinyBERT_Distilling')">BibTex</button-->
    
    <a href="https://arxiv.org/abs/1909.10351">[PDF]</a>
    
<!--div class="bibtex" id='JiaoYinEtAl_2019_TinyBERT_Distilling'><pre>@article{JiaoYinEtAl_2019_TinyBERT_Distilling_BERT_for_Natural_Language_Understanding,
  title = {{{TinyBERT}}: {{Distilling BERT}} for {{Natural Language Understanding}}},
  shorttitle = {{{TinyBERT}}},
  journal = {arXiv preprint arXiv:1909.10351},
  author = {Jiao, Xiaoqi and Yin, Yichun and Shang, Lifeng and Jiang, Xin and Chen, Xiao and Li, Linlin and Wang, Fang and Liu, Qun},
  url = {https://arxiv.org/abs/1909.10351},
  year = {2019}
}
</pre>
https://arxiv.org/abs/1909.10351
</div-->
</div>



<div>
    
</div></li>
<li><div class="text-justify">
    <span id="JawaharSagotEtAl_2019_What_does_BERT_learn_about_structure_of_language">Jawahar, G., Sagot, B., &amp; Seddah, D. (2019). What Does BERT Learn about the Structure of Language? <i>ACL 2019</i>, 3651–3657. Florence, Italy.</span>

    
    

    <!--button class="btn--info" onclick="showBibtex('JawaharSagotEtAl_2019_What_does')">BibTex</button-->
    
    <a href="https://www.aclweb.org/anthology/P19-1356.pdf">[PDF]</a>
    
<!--div class="bibtex" id='JawaharSagotEtAl_2019_What_does'><pre>@inproceedings{JawaharSagotEtAl_2019_What_does_BERT_learn_about_structure_of_language,
  address = {{Florence, Italy}},
  title = {What Does {{BERT}} Learn about the Structure of Language?},
  booktitle = {{{ACL}} 2019},
  url = {https://www.aclweb.org/anthology/P19-1356.pdf},
  author = {Jawahar, Ganesh and Sagot, Beno{\^i}t and Seddah, Djam{\'e}},
  year = {2019},
  pages = {3651--3657}
}
</pre>
https://www.aclweb.org/anthology/P19-1356.pdf
</div-->
</div>



<div>
    
</div></li>
<li><div class="text-justify">
    <span id="HewittManning_2019_Structural_Probe_for_Finding_Syntax_in_Word_Representations">Hewitt, J., &amp; Manning, C. D. (2019). A Structural Probe for Finding Syntax in Word Representations. <i>Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)</i>, 4129–4138.</span>

    
    

    <!--button class="btn--info" onclick="showBibtex('HewittManning_2019_Structural_Probe')">BibTex</button-->
    
    <a href="https://aclweb.org/anthology/papers/N/N19/N19-1419/">[PDF]</a>
    
<!--div class="bibtex" id='HewittManning_2019_Structural_Probe'><pre>@inproceedings{HewittManning_2019_Structural_Probe_for_Finding_Syntax_in_Word_Representations,
  title = {A {{Structural Probe}} for {{Finding Syntax}} in {{Word Representations}}},
  booktitle = {Proceedings of the 2019 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}, {{Volume}} 1 ({{Long}} and {{Short Papers}})},
  url = {https://aclweb.org/anthology/papers/N/N19/N19-1419/},
  author = {Hewitt, John and Manning, Christopher D.},
  year = {2019},
  pages = {4129-4138}
}
</pre>
https://aclweb.org/anthology/papers/N/N19/N19-1419/
</div-->
</div>



<div>
    
</div></li>
<li><div class="text-justify">
    <span id="GururanganSwayamdiptaEtAl_2018_Annotation_Artifacts_in_Natural_Language_Inference_Data">Gururangan, S., Swayamdipta, S., Levy, O., Schwartz, R., Bowman, S., &amp; Smith, N. A. (2018). Annotation Artifacts in Natural Language Inference Data. <i>Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)</i>, 107–112. https://doi.org/10.18653/v1/N18-2017</span>

    
    

    <!--button class="btn--info" onclick="showBibtex('GururanganSwayamdiptaEtAl_2018_Annotation_Artifacts')">BibTex</button-->
    
    <a href="https://www.aclweb.org/anthology/N18-2017/">[PDF]</a>
    
<!--div class="bibtex" id='GururanganSwayamdiptaEtAl_2018_Annotation_Artifacts'><pre>@inproceedings{GururanganSwayamdiptaEtAl_2018_Annotation_Artifacts_in_Natural_Language_Inference_Data,
  address = {{New Orleans, Louisiana}},
  title = {Annotation {{Artifacts}} in {{Natural Language Inference Data}}},
  booktitle = {Proceedings of the 2018 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}, {{Volume}} 2 ({{Short Papers}})},
  publisher = {{Association for Computational Linguistics}},
  doi = {10.18653/v1/N18-2017},
  author = {Gururangan, Suchin and Swayamdipta, Swabha and Levy, Omer and Schwartz, Roy and Bowman, Samuel and Smith, Noah A.},
  year = {2018},
  url = {https://www.aclweb.org/anthology/N18-2017/},
  pages = {107--112},
  ids = {GururanganSwayamdiptaEtAl\_2018\_Annotation\_Artifacts\_in\_Natural\_Language\_Inference\_Data}
}
</pre>
https://www.aclweb.org/anthology/N18-2017/
</div-->
</div>



<div>
    
</div></li>
<li><div class="text-justify">
    <span id="Goldberg_2019_Assessing_BERTs_Syntactic_Abilities">Goldberg, Y. (2019). Assessing BERT’s Syntactic Abilities. <i>ArXiv:1901.05287 [Cs]</i>.</span>

    
    

    <!--button class="btn--info" onclick="showBibtex('Goldberg_2019_Assessing_BERTs')">BibTex</button-->
    
    <a href="http://arxiv.org/abs/1901.05287">[PDF]</a>
    
<!--div class="bibtex" id='Goldberg_2019_Assessing_BERTs'><pre>@article{Goldberg_2019_Assessing_BERTs_Syntactic_Abilities,
  archiveprefix = {arXiv},
  primaryclass = {cs},
  title = {Assessing {{BERT}}'s {{Syntactic Abilities}}},
  journal = {arXiv:1901.05287 [cs]},
  url = {http://arxiv.org/abs/1901.05287},
  author = {Goldberg, Yoav},
  year = {2019}
}
</pre>
http://arxiv.org/abs/1901.05287
</div-->
</div>



<div>
    
</div></li>
<li><div class="text-justify">
    <span id="Ettinger_2019_What_BERT_is_not_Lessons_from_new_suite_of_psycholinguistic_diagnostics_for_language_models">Ettinger, A. (2019). What BERT Is Not: Lessons from a New Suite of Psycholinguistic Diagnostics for Language Models. <i>ArXiv:1907.13528 [Cs]</i>.</span>

    
    

    <!--button class="btn--info" onclick="showBibtex('Ettinger_2019_What_BERT')">BibTex</button-->
    
    <a href="http://arxiv.org/abs/1907.13528">[PDF]</a>
    
<!--div class="bibtex" id='Ettinger_2019_What_BERT'><pre>@article{Ettinger_2019_What_BERT_is_not_Lessons_from_new_suite_of_psycholinguistic_diagnostics_for_language_models,
  archiveprefix = {arXiv},
  primaryclass = {cs},
  title = {What {{BERT}} Is Not: {{Lessons}} from a New Suite of Psycholinguistic Diagnostics for Language Models},
  shorttitle = {What {{BERT}} Is Not},
  journal = {arXiv:1907.13528 [cs]},
  url = {http://arxiv.org/abs/1907.13528},
  author = {Ettinger, Allyson},
  year = {2019}
}
</pre>
http://arxiv.org/abs/1907.13528
</div-->
</div>



<div>
    
</div></li>
<li><div class="text-justify">
    <span id="DevlinChangEtAl_2019_BERT_Pre-training_of_Deep_Bidirectional_Transformers_for_Language_Understanding">Devlin, J., Chang, M.-W., Lee, K., &amp; Toutanova, K. (2019). BERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding. <i>Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)</i>, 4171–4186.</span>

    
    

    <!--button class="btn--info" onclick="showBibtex('DevlinChangEtAl_2019_BERT_Pre-training')">BibTex</button-->
    
    <a href="https://aclweb.org/anthology/papers/N/N19/N19-1423/">[PDF]</a>
    
<!--div class="bibtex" id='DevlinChangEtAl_2019_BERT_Pre-training'><pre>@inproceedings{DevlinChangEtAl_2019_BERT_Pre-training_of_Deep_Bidirectional_Transformers_for_Language_Understanding,
  title = {{{BERT}}: {{Pre}}-Training of {{Deep Bidirectional Transformers}} for {{Language Understanding}}},
  shorttitle = {{{BERT}}},
  booktitle = {Proceedings of the 2019 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}, {{Volume}} 1 ({{Long}} and {{Short Papers}})},
  url = {https://aclweb.org/anthology/papers/N/N19/N19-1423/},
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  year = {2019},
  pages = {4171-4186}
}
</pre>
https://aclweb.org/anthology/papers/N/N19/N19-1423/
</div-->
</div>



<div>
    
</div></li>
<li><div class="text-justify">
    <span id="BakerFillmoreEtAl_1998_Berkeley_Framenet_project">Baker, C. F., Fillmore, C. J., &amp; Lowe, J. B. (1998). The Berkeley Framenet Project. <i>Proceedings of the 17th International Conference on Computational Linguistics</i>, <i>1</i>, 86–90. Association for Computational Linguistics.</span>

    
    

    <!--button class="btn--info" onclick="showBibtex('BakerFillmoreEtAl_1998_Berkeley_Framenet')">BibTex</button-->
    
    <a href="https://www.aclweb.org/anthology/C98-1013/">[PDF]</a>
    
<!--div class="bibtex" id='BakerFillmoreEtAl_1998_Berkeley_Framenet'><pre>@inproceedings{BakerFillmoreEtAl_1998_Berkeley_Framenet_project,
  title = {The {{Berkeley Framenet}} Project},
  volume = {1},
  booktitle = {Proceedings of the 17th International Conference on {{Computational Linguistics}}},
  publisher = {{Association for Computational Linguistics}},
  author = {Baker, Collin F. and Fillmore, Charles J. and Lowe, John B.},
  year = {1998},
  url = {https://www.aclweb.org/anthology/C98-1013/},
  pages = {86--90}
}
</pre>
https://www.aclweb.org/anthology/C98-1013/
</div-->
</div>



<div>
    
</div></li></ol>


        </section>

        <aside id="sidebar">
          
            <ul class="toc">
  <li><a href="#">The Dark Secrets of BERT</a>
    <ul>
      <li><a href="#a-brief-intro-to-bert">A brief intro to BERT</a></li>
      <li><a href="#what-types-of-self-attention-patterns-are-learned-and-how-many-of-each-type">What types of self-attention patterns are learned, and how many of each type?</a></li>
      <li><a href="#what-happens-in-fine-tuning">What happens in fine-tuning?</a></li>
      <li><a href="#how-much-difference-does-fine-tuning-make">How much difference does fine-tuning make?</a></li>
      <li><a href="#are-there-any-linguistically-interpretable-self-attention-heads">Are there any linguistically interpretable self-attention heads?</a></li>
      <li><a href="#but-what-information-actually-gets-used-at-inference-time">But… what information actually gets used at inference time?</a></li>
      <li><a href="#so-how-does-this-thing-work">So how does this thing work?</a></li>
      <li><a href="#"> References </a></li>
    </ul>
  </li>
</ul>
          
        </aside>


      </div>
    </div>

    
  </body>
</html>
